{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb06546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import & Konfigurasi\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Konfigurasi File\n",
    "INPUT_FILE = 'Data/Data_Siap_Mining.csv' # Sesuaikan path jika perlu\n",
    "OUTPUT_FILE = 'Final_Skripsi_Rules.csv'\n",
    "\n",
    "# Konfigurasi Algoritma\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "print(\"‚úÖ Library loaded & Config set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cd64d",
   "metadata": {},
   "source": [
    "# Analisis Anomali Network Menggunakan FP-Growth dan Association Rules\n",
    "\n",
    "## Deskripsi Proyek\n",
    "Notebook ini menganalisis log jaringan untuk mendeteksi anomali menggunakan algoritma **FP-Growth** (Frequent Pattern Growth) dan **Association Rules**. Tujuan utama adalah menemukan pola hubungan antara kondisi jaringan dan diagnosis anomali.\n",
    "\n",
    "## Struktur Analisis\n",
    "1. **Encoding Data**: Mengonversi data log menjadi format transaksi yang dapat dianalisis\n",
    "2. **FP-Growth Algorithm**: Menemukan pola frekuen dengan threshold minimum support\n",
    "3. **Association Rules**: Menghasilkan aturan asosiasi untuk prediksi diagnosis\n",
    "4. **Filtering & Visualization**: Menyaring aturan yang relevan dan menampilkan hasil akhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b63ad",
   "metadata": {},
   "source": [
    "## Tahap 1: Import Library dan Persiapan Data\n",
    "\n",
    "Pada tahap ini, kami mengimpor library yang diperlukan untuk analisis frequent pattern mining dan memuat dataset yang sudah dibersihkan dari tahap preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c967ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Import cleaned data dari hasil preprocessing\n",
    "df_cleaned = pd.read_csv('Data/Data_Siap_Mining.csv')\n",
    "print(f\"Dataset berhasil dimuat: {df_cleaned.shape[0]} baris, {df_cleaned.shape[1]} kolom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09efe3",
   "metadata": {},
   "source": [
    "## Tahap 2: Encoding Data dan Perhitungan FP-Growth\n",
    "\n",
    "### Proses Encoding\n",
    "- **TransactionEncoder**: Mengkonversi data log dari format text menjadi format transaksi (one-hot encoded)\n",
    "- Setiap item dalam 'items' column diubah menjadi kolom boolean terpisah\n",
    "- Output: DataFrame `df_encoded` dengan nilai True/False untuk setiap atribut\n",
    "\n",
    "### Perhitungan Support\n",
    "- **Minimum Support**: Threshold minimum untuk pola yang dianggap \"sering\" terjadi\n",
    "- **MIN_SUPPORT_COUNT = 20**: Pola harus muncul minimal 20 kali dalam dataset\n",
    "- **min_support_pct**: Persentase support dihitung dari total jumlah transaksi\n",
    "\n",
    "### Algoritma FP-Growth\n",
    "- **fpgrowth**: Algoritma efisien untuk mining pola frekuen\n",
    "- Lebih cepat dari Apriori karena menggunakan FP-tree structure\n",
    "- Output: `frequent_itemsets` berisi semua pola yang memenuhi threshold\n",
    "\n",
    "### Association Rules\n",
    "- Menghasilkan aturan IF-THEN dari pola frekuen\n",
    "- **min_threshold = 0.6**: Hanya aturan dengan confidence ‚â• 60% yang dipertahankan\n",
    "- Metrik yang dihitung: support, confidence, lift, dsb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Encoding & Mining (REVISI)\n",
    "# 1. Encoding (Ubah List jadi Matriks Boolean)\n",
    "print(\"‚è≥ Encoding data...\")\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# Hapus list transaksi segera\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Hapus array numpy segera\n",
    "del te_ary\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Encoding Selesai. Shape: {df_encoded.shape}\")\n",
    "\n",
    "# 2. Hitung Threshold Support\n",
    "min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "print(f\"üîç Mining dengan Min Support: {min_support_pct:.5f} (Min {MIN_SUPPORT_COUNT} kejadian)\")\n",
    "\n",
    "# 3. Jalankan FP-Growth (Cari Pola)\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support_pct, use_colnames=True)\n",
    "\n",
    "# Hapus matriks besar segera (Hemat RAM)\n",
    "del df_encoded\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Mining Selesai. Ditemukan {len(frequent_itemsets)} pola.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: FP-GROWTH PER DIAGNOSIS LABEL =====\n",
    "# Jalankan FP-Growth untuk masing-masing diagnosis label\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "\n",
    "frequent_itemsets_dict = {}\n",
    "print(f\"‚úì Menjalankan FP-Growth per diagnosis label...\")\n",
    "print(f\"  - MIN_SUPPORT_COUNT: {MIN_SUPPORT_COUNT}\\n\")\n",
    "\n",
    "for label, df_encoded in encoded_data_per_label.items():\n",
    "    print(f\"FP-Growth for '{label}'...\", end=\" \")\n",
    "    \n",
    "    # Hitung minimum support percentage\n",
    "    min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "    \n",
    "    # Gunakan hanya kolom atribut (exclude diagnosis column)\n",
    "    df_for_fpgrowth = df_encoded.drop('diagnosis', axis=1)\n",
    "    \n",
    "    # Jalankan FP-Growth\n",
    "    frequent_itemsets = fpgrowth(df_for_fpgrowth, min_support=min_support_pct, use_colnames=True)\n",
    "    \n",
    "    # Tambahkan diagnosis label ke dalam frequent itemsets\n",
    "    frequent_itemsets['diagnosis'] = label\n",
    "    \n",
    "    frequent_itemsets_dict[label] = frequent_itemsets\n",
    "    \n",
    "    print(f\"‚úì Ditemukan {len(frequent_itemsets)} pola frekuen\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del df_encoded, df_for_fpgrowth\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì FP-Growth selesai untuk semua labels!\")\n",
    "\n",
    "# Gabungkan semua frequent itemsets\n",
    "all_frequent_itemsets = pd.concat(frequent_itemsets_dict.values(), ignore_index=True)\n",
    "print(f\"  - Total pola frekuen: {len(all_frequent_itemsets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: GENERATE ASSOCIATION RULES PER LABEL =====\n",
    "# Buat association rules untuk setiap diagnosis label\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "rules_dict = {}\n",
    "total_rules = 0\n",
    "\n",
    "print(f\"‚úì Menghasilkan Association Rules per diagnosis label...\")\n",
    "print(f\"  - MIN_CONFIDENCE: {MIN_CONFIDENCE}\\n\")\n",
    "\n",
    "for label, frequent_itemsets in frequent_itemsets_dict.items():\n",
    "    print(f\"Generating rules for '{label}'...\", end=\" \")\n",
    "    \n",
    "    if len(frequent_itemsets) == 0:\n",
    "        print(\"(Skip - tidak ada frequent itemsets)\")\n",
    "        continue\n",
    "    \n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "    \n",
    "    if len(rules) > 0:\n",
    "        rules['diagnosis'] = label\n",
    "        rules_dict[label] = rules\n",
    "        total_rules += len(rules)\n",
    "        print(f\"‚úì Ditemukan {len(rules)} rules\")\n",
    "    else:\n",
    "        print(\"(tidak ada rules dengan confidence >= threshold)\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del frequent_itemsets\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì Association Rules generation selesai!\")\n",
    "print(f\"  - Total rules: {total_rules}\")\n",
    "\n",
    "# Gabungkan semua rules\n",
    "if rules_dict:\n",
    "    all_rules = pd.concat(rules_dict.values(), ignore_index=True)\n",
    "    print(f\"  - Rules yang digabungkan: {len(all_rules)}\")\n",
    "else:\n",
    "    all_rules = pd.DataFrame()\n",
    "    print(\"  - Tidak ada rules yang dihasilkan!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e002",
   "metadata": {},
   "source": [
    "## Tahap 3: Penyaringan dan Tampilan Hasil Akhir\n",
    "\n",
    "### Konsep Filtering\n",
    "Dari semua association rules yang dihasilkan, kita hanya ingin menyimpan rules yang:\n",
    "- **Consequent (Kesimpulan)**: Harus berupa diagnosis/label anomali\n",
    "  - NORMAL: Kondisi jaringan normal\n",
    "  - UPSTREAM_FAILURE: Kegagalan uplink\n",
    "  - LINK_FAILURE: Kegagalan link\n",
    "  - DDOS_ATTACK: Serangan DDoS\n",
    "  - BROADCAST_STORM: Broadcast storm\n",
    "\n",
    "Contoh Rule yang BENAR (yang kita inginkan):\n",
    "- IF `ether1=down AND queue=high` THEN `LINK_FAILURE` ‚úì\n",
    "\n",
    "Contoh Rule yang SALAH (yang kita buang):\n",
    "- IF `LINK_FAILURE` THEN `ether1=down` ‚úó\n",
    "\n",
    "### Metrik Evaluasi\n",
    "- **Support**: Seberapa sering pola ini muncul dalam dataset (0-1)\n",
    "- **Confidence**: Probabilitas consequent terjadi jika antecedent terjadi (0-1)\n",
    "- **Lift**: Seberapa kuat hubungan antara antecedent dan consequent (>1 = hubungan positif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15793906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: FILTERING RULES =====\n",
    "# Filter rules - hanya ambil yang berakhir dengan diagnosis\n",
    "def is_diagnosis(consequents):\n",
    "    \"\"\"\n",
    "    Cek apakah consequents (frozenset) mengandung minimal 1 label diagnosis\n",
    "    \"\"\"\n",
    "    return any(label in consequents for label in DIAGNOSIS_LABELS)\n",
    "\n",
    "if len(all_rules) > 0:\n",
    "    final_rules = all_rules[all_rules['consequents'].apply(is_diagnosis)].copy()\n",
    "    print(f\"‚úì Rules setelah filtering:\")\n",
    "    print(f\"  - Total rules sebelum filter: {len(all_rules)}\")\n",
    "    print(f\"  - Rules dengan diagnosis consequent: {len(final_rules)}\")\n",
    "    print(f\"  - Rules yang dihilangkan: {len(all_rules) - len(final_rules)}\")\n",
    "    \n",
    "    # ===== STEP 5: SORTING =====\n",
    "    # Urutkan berdasarkan confidence (tertinggi) kemudian support (tertinggi)\n",
    "    final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "    print(f\"\\n‚úì Rules sudah diurutkan berdasarkan confidence dan support (descending)\")\n",
    "    \n",
    "    # ===== STEP 6: DISPLAY HASIL =====\n",
    "    # Konversi frozenset ke list untuk memudahkan pembacaan CSV dan Streamlit\n",
    "    final_rules_for_display = final_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'diagnosis']].copy()\n",
    "    final_rules_for_display['antecedents'] = final_rules_for_display['antecedents'].apply(list)\n",
    "    final_rules_for_display['consequents'] = final_rules_for_display['consequents'].apply(list)\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"TOP ASSOCIATION RULES - NETWORK ANOMALY DETECTION (PROCESSED PER LABEL)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Tampilkan dengan format yang lebih rapi (hanya 15 untuk display)\n",
    "    for idx, (i, row) in enumerate(final_rules_for_display.head(15).iterrows(), 1):\n",
    "        antecedents = ', '.join(row['antecedents'])\n",
    "        consequents = ', '.join(row['consequents'])\n",
    "        diagnosis = row['diagnosis']\n",
    "        \n",
    "        print(f\"[Rule {idx}] [{diagnosis}]\")\n",
    "        print(f\"  IF:   {antecedents}\")\n",
    "        print(f\"  THEN: {consequents}\")\n",
    "        print(f\"  Support: {row['support']:.4f} | Confidence: {row['confidence']:.4f} | Lift: {row['lift']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # ===== STEP 7: SAVE HASIL =====\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"MENYIMPAN DATA KE CSV...\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Simpan SEMUA final rules ke CSV\n",
    "    final_rules_for_display.to_csv('../Final_Skripsi_Rules.csv', index=False)\n",
    "    print(f\"‚úì SEMUA {len(final_rules_for_display)} rules berhasil disimpan ke 'Final_Skripsi_Rules.csv'\")\n",
    "    print(f\"  - File ini akan digunakan oleh Dashboard Live Monitoring\")\n",
    "    print(f\"  - Frozenset sudah dikonversi ke list untuk kompatibilitas Streamlit\")\n",
    "    print(f\"  - Format: antecedents | consequents | support | confidence | lift | diagnosis\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"‚úì TOP 15 RULES (sample dari {len(final_rules_for_display)} total rules)\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del all_rules, all_frequent_itemsets\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† Tidak ada rules yang dihasilkan dari semua labels!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc1373",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Interpretasi\n",
    "\n",
    "### Hasil Analisis\n",
    "Notebook ini menghasilkan association rules yang menunjukkan hubungan antara kondisi jaringan dan tipe anomali. Setiap rule memiliki:\n",
    "- **Antecedents (IF)**: Kondisi atau kombinasi kondisi jaringan yang diamati\n",
    "- **Consequents (THEN)**: Diagnosis/prediksi tipe anomali yang kemungkinan terjadi\n",
    "- **Confidence**: Tingkat akurasi prediksi (semakin tinggi semakin baik)\n",
    "- **Support**: Seberapa sering pola ini muncul dalam data (frekuensi)\n",
    "- **Lift**: Seberapa kuat hubungan antara kondisi dan diagnosis\n",
    "\n",
    "### Penggunaan Praktis\n",
    "Hasil rules ini dapat digunakan untuk:\n",
    "1. **Early Warning System**: Mendeteksi anomali sebelum terjadi dengan mengecek kondisi antecedent\n",
    "2. **Root Cause Analysis**: Memahami faktor-faktor apa yang menyebabkan setiap tipe anomali\n",
    "3. **Network Optimization**: Meningkatkan monitoring dan maintenance protokol berdasarkan insights\n",
    "4. **Decision Support**: Membantu network administrator membuat keputusan yang lebih cepat dan tepat\n",
    "\n",
    "### File Output\n",
    "- **Final_Skripsi_Rules.csv**: Berisi top 10 rules dalam format CSV yang dapat digunakan untuk proses selanjutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data & Gabung Label (REVISI)\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Konfigurasi\n",
    "INPUT_FILE = 'Data/Data_Siap_Mining.csv'\n",
    "OUTPUT_FILE = 'Final_Skripsi_Rules.csv'\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"üìä Dataset dimuat: {len(df)} baris.\")\n",
    "\n",
    "# 2. Deteksi Nama Kolom Label (diagnosis atau Label)\n",
    "label_col = 'Label' if 'Label' in df.columns else 'diagnosis'\n",
    "print(f\"‚ÑπÔ∏è Kolom Target: {label_col}\")\n",
    "\n",
    "# 3. GABUNGKAN Gejala + Label (KUNCI UTAMA AGAR BERHASIL)\n",
    "# Kita memasukkan Label Diagnosis ke dalam keranjang belanja yang sama dengan gejalanya.\n",
    "transactions = []\n",
    "for _, row in df.iterrows():\n",
    "    # Parse string list kembali ke list python\n",
    "    items = ast.literal_eval(row['items']) if isinstance(row['items'], str) else row['items']\n",
    "    # Tambahkan Label Diagnosis ke dalam list items\n",
    "    items.append(row[label_col]) \n",
    "    transactions.append(items)\n",
    "\n",
    "# Hapus DF awal buat hemat RAM\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Transaksi Siap (Contoh: {transactions[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Generate Rules & Filtering (REVISI)\n",
    "# 1. Generate Rules (Cari Hubungan Sebab-Akibat)\n",
    "print(\"‚öôÔ∏è Generating Rules...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "del frequent_itemsets\n",
    "gc.collect()\n",
    "\n",
    "# 2. Filtering (Hanya Simpan Rule yang Ujungnya Diagnosis)\n",
    "TARGET_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "def is_diagnosis_rule(consequents):\n",
    "    return any(label in consequents for label in TARGET_LABELS)\n",
    "\n",
    "final_rules = rules[rules['consequents'].apply(is_diagnosis_rule)].copy()\n",
    "\n",
    "# 3. Formatting untuk CSV\n",
    "# Ubah format frozenset ({'a'}) jadi list biasa ['a'] biar bisa dibaca Dashboard\n",
    "final_rules['antecedents'] = final_rules['antecedents'].apply(lambda x: list(x))\n",
    "final_rules['consequents'] = final_rules['consequents'].apply(lambda x: list(x))\n",
    "\n",
    "# Sort dari yang paling akurat\n",
    "final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "# 4. Simpan ke CSV\n",
    "final_rules.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"‚úÖ SUKSES! Total Rules Valid: {len(final_rules)}\")\n",
    "print(f\"üíæ File Rules tersimpan di: {OUTPUT_FILE}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Preview Top 5\n",
    "print(\"\\nTop 5 Rules Terbaik:\")\n",
    "for idx, row in final_rules.head(5).iterrows():\n",
    "    print(f\"IF {row['antecedents']} THEN {row['consequents']} (Conf: {row['confidence']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FP-Growth Mining\n",
    "# 1. Hitung Support Percentage\n",
    "min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "print(f\"üîç Mining dengan Min Support: {min_support_pct:.5f} (Setara {MIN_SUPPORT_COUNT} baris)\")\n",
    "\n",
    "# 2. Jalankan FP-Growth\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support_pct, use_colnames=True)\n",
    "\n",
    "# 3. Hapus matriks besar secepatnya (Paling makan RAM)\n",
    "del df_encoded\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Mining selesai. Ditemukan {len(frequent_itemsets)} pola itemset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e58ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate Rules & Filtering (Final)\n",
    "# 1. Generate Rules\n",
    "print(\"‚öôÔ∏è Sedang generate Association Rules...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# 2. Hapus itemsets\n",
    "del frequent_itemsets\n",
    "gc.collect()\n",
    "\n",
    "# 3. Filtering: Hanya ambil rule yang berujung DIAGNOSIS\n",
    "TARGET_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "def is_diagnosis_rule(consequents):\n",
    "    # Cek apakah hasil (THEN) adalah salah satu label diagnosis\n",
    "    return any(label in consequents for label in TARGET_LABELS)\n",
    "\n",
    "# Terapkan filter\n",
    "final_rules = rules[rules['consequents'].apply(is_diagnosis_rule)].copy()\n",
    "\n",
    "# 4. Sorting & Formatting\n",
    "final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "# Ubah frozenset jadi list biasa agar bersih di CSV\n",
    "final_rules['antecedents'] = final_rules['antecedents'].apply(lambda x: list(x))\n",
    "final_rules['consequents'] = final_rules['consequents'].apply(lambda x: list(x))\n",
    "\n",
    "# 5. Simpan\n",
    "final_rules.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"‚úÖ SUKSES! Total Rules Valid: {len(final_rules)}\")\n",
    "print(f\"üíæ Disimpan ke: {OUTPUT_FILE}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Preview Top 5\n",
    "print(\"\\nTop 5 Rules Terbaik:\")\n",
    "for idx, row in final_rules.head(5).iterrows():\n",
    "    print(f\"IF {row['antecedents']} THEN {row['consequents']} (Conf: {row['confidence']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
