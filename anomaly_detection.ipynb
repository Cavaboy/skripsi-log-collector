{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482cd64d",
   "metadata": {},
   "source": [
    "# Analisis Anomali Network Menggunakan FP-Growth dan Association Rules\n",
    "\n",
    "## Deskripsi Proyek\n",
    "Notebook ini menganalisis log jaringan untuk mendeteksi anomali menggunakan algoritma **FP-Growth** (Frequent Pattern Growth) dan **Association Rules**. Tujuan utama adalah menemukan pola hubungan antara kondisi jaringan dan diagnosis anomali.\n",
    "\n",
    "## Struktur Analisis\n",
    "1. **Encoding Data**: Mengonversi data log menjadi format transaksi yang dapat dianalisis\n",
    "2. **FP-Growth Algorithm**: Menemukan pola frekuen dengan threshold minimum support\n",
    "3. **Association Rules**: Menghasilkan aturan asosiasi untuk prediksi diagnosis\n",
    "4. **Filtering & Visualization**: Menyaring aturan yang relevan dan menampilkan hasil akhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b63ad",
   "metadata": {},
   "source": [
    "## Tahap 1: Import Library dan Persiapan Data\n",
    "\n",
    "Pada tahap ini, kami mengimpor library yang diperlukan untuk analisis frequent pattern mining dan memuat dataset yang sudah dibersihkan dari tahap preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb06546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library & Konfigurasi Siap.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Konfigurasi Global\n",
    "INPUT_FILE = 'Data/Data_Siap_Mining.csv'\n",
    "OUTPUT_FILE = 'Final_Skripsi_Rules.csv'\n",
    "MIN_SUPPORT_COUNT = 30\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "print(\"Library & Konfigurasi Siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09efe3",
   "metadata": {},
   "source": [
    "## Tahap 2: Encoding Data dan Perhitungan FP-Growth\n",
    "\n",
    "### Proses Encoding\n",
    "- **TransactionEncoder**: Mengkonversi data log dari format text menjadi format transaksi (one-hot encoded)\n",
    "- Setiap item dalam 'items' column diubah menjadi kolom boolean terpisah\n",
    "- Output: DataFrame `df_encoded` dengan nilai True/False untuk setiap atribut\n",
    "\n",
    "### Perhitungan Support\n",
    "- **Minimum Support**: Threshold minimum untuk pola yang dianggap \"sering\" terjadi\n",
    "- **MIN_SUPPORT_COUNT = 20**: Pola harus muncul minimal 20 kali dalam dataset\n",
    "- **min_support_pct**: Persentase support dihitung dari total jumlah transaksi\n",
    "\n",
    "### Algoritma FP-Growth\n",
    "- **fpgrowth**: Algoritma efisien untuk mining pola frekuen\n",
    "- Lebih cepat dari Apriori karena menggunakan FP-tree structure\n",
    "- Output: `frequent_itemsets` berisi semua pola yang memenuhi threshold\n",
    "\n",
    "### Association Rules\n",
    "- Menghasilkan aturan IF-THEN dari pola frekuen\n",
    "- **min_threshold = 0.6**: Hanya aturan dengan confidence ≥ 60% yang dipertahankan\n",
    "- Metrik yang dihitung: support, confidence, lift, dsb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaba59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset dimuat: 6537 baris, 2 kolom\n",
      " Kolom Label: Label\n",
      "Transaksi siap. Contoh baris pertama: ['r1-core', 'system', 'router', 'rebooted', 'console', 'admin', 'ttys0', 'shutdown', 'NORMAL']\n",
      "Total transaksi: 6537\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load Data & Gabung Label (CRITICAL STEP)\n",
    "# 1. Load the dataset from INPUT_FILE\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\" Dataset dimuat: {len(df)} baris, {df.shape[1]} kolom\")\n",
    "\n",
    "# 2. Determine if the label column is named 'Label' or 'diagnosis'\n",
    "label_col = 'Label' if 'Label' in df.columns else 'diagnosis'\n",
    "print(f\" Kolom Label: {label_col}\")\n",
    "\n",
    "# 3. Create a list called 'transactions'\n",
    "# 4. For each row, parse the 'items' string using ast.literal_eval (NOT eval)\n",
    "# 5. CRITICAL: Append the row's Label/diagnosis value into the items list\n",
    "# 6. Add the list to 'transactions'\n",
    "transactions = []\n",
    "for _, row in df.iterrows():\n",
    "    # Parse string list kembali ke list python menggunakan ast.literal_eval (AMAN)\n",
    "    items = ast.literal_eval(row['items']) if isinstance(row['items'], str) else row['items']\n",
    "    # CRITICAL: Masukkan Label ke dalam items list (INI KUNCINYA!)\n",
    "    items.append(row[label_col])\n",
    "    transactions.append(items)\n",
    "\n",
    "# 7. Delete the dataframe and run gc.collect() to free up memory\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# 8. Print the first transaction to verify the label is included\n",
    "print(f\"Transaksi siap. Contoh baris pertama: {transactions[0]}\")\n",
    "print(f\"Total transaksi: {len(transactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d554d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang melakukan Encoding...\n",
      "Encoding Selesai. Shape: (6537, 132)\n",
      "Mining dengan Min Support: 0.00459 (Min 30 kejadian)\n",
      "Mining Selesai. Ditemukan 630334 pola itemset.\n",
      "Cek Jumlah Pola: 630334\n",
      "Terlalu banyak pola (>500rb).\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Encoding & FP-Growth\n",
    "# 1. Initialize TransactionEncoder. Fit and transform 'transactions' into a boolean array\n",
    "print(\"Sedang melakukan Encoding...\")\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# 2. Delete 'transactions' list and run gc.collect() immediately to save RAM\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "# 3. Create a DataFrame 'df_encoded' from the array. Delete the array and run gc.collect()\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "del te_ary\n",
    "gc.collect()\n",
    "\n",
    "# 4. Calculate 'min_support_pct' by dividing MIN_SUPPORT_COUNT by the length of df_encoded\n",
    "min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "print(f\"Encoding Selesai. Shape: {df_encoded.shape}\")\n",
    "print(f\"Mining dengan Min Support: {min_support_pct:.5f} (Min {MIN_SUPPORT_COUNT} kejadian)\")\n",
    "\n",
    "# 5. Run fpgrowth on 'df_encoded' using 'min_support_pct' and use_colnames=True\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support_pct, use_colnames=True)\n",
    "\n",
    "# 6. Delete 'df_encoded' and run gc.collect()\n",
    "del df_encoded\n",
    "gc.collect()\n",
    "\n",
    "# 7. Print the number of frequent itemsets found\n",
    "print(f\"Mining Selesai. Ditemukan {len(frequent_itemsets)} pola itemset.\")\n",
    "print(f\"Cek Jumlah Pola: {len(frequent_itemsets)}\")\n",
    "if len(frequent_itemsets) > 500000:\n",
    "    print(\"Terlalu banyak pola (>500rb).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e002",
   "metadata": {},
   "source": [
    "## Tahap 3: Penyaringan dan Tampilan Hasil Akhir\n",
    "\n",
    "### Konsep Filtering\n",
    "Dari semua association rules yang dihasilkan, kita hanya ingin menyimpan rules yang:\n",
    "- **Consequent (Kesimpulan)**: Harus berupa diagnosis/label anomali\n",
    "  - NORMAL: Kondisi jaringan normal\n",
    "  - UPSTREAM_FAILURE: Kegagalan uplink\n",
    "  - LINK_FAILURE: Kegagalan link\n",
    "  - DDOS_ATTACK: Serangan DDoS\n",
    "  - BROADCAST_STORM: Broadcast storm\n",
    "\n",
    "Contoh Rule yang BENAR (yang kita inginkan):\n",
    "- IF `ether1=down AND queue=high` THEN `LINK_FAILURE` ✓\n",
    "\n",
    "Contoh Rule yang SALAH (yang kita buang):\n",
    "- IF `LINK_FAILURE` THEN `ether1=down` ✗\n",
    "\n",
    "### Metrik Evaluasi\n",
    "- **Support**: Seberapa sering pola ini muncul dalam dataset (0-1)\n",
    "- **Confidence**: Probabilitas consequent terjadi jika antecedent terjadi (0-1)\n",
    "- **Lift**: Seberapa kuat hubungan antara antecedent dan consequent (>1 = hubungan positif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b636bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Rules (Min Conf: 0.6)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# CELL 4: Generate Rules, Filter & Save\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 1. Generate association rules using metric=\"confidence\" and min_threshold=MIN_CONFIDENCE\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating Rules (Min Conf: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIN_CONFIDENCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m rules = \u001b[43massociation_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrequent_itemsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfidence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMIN_CONFIDENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Delete frequent_itemsets to free memory\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m frequent_itemsets\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\mlxtend\\frequent_patterns\\association_rules.py:361\u001b[39m, in \u001b[36massociation_rules\u001b[39m\u001b[34m(df, num_itemsets, df_orig, null_values, metric, min_threshold, support_only, return_metrics)\u001b[39m\n\u001b[32m    359\u001b[39m                 rule_antecedents.append(antecedent)\n\u001b[32m    360\u001b[39m                 rule_consequents.append(consequent)\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m                 rule_supports.append(\n\u001b[32m    362\u001b[39m                     [sAC, sA, sC, disAC, disA, disC, dis_int, dis_int_]\n\u001b[32m    363\u001b[39m                 )\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# check if frequent rule was generated\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rule_supports:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# CELL 4: Generate Rules, Filter & Save\n",
    "# 1. Generate association rules using metric=\"confidence\" and min_threshold=MIN_CONFIDENCE\n",
    "print(f\"\\nGenerating Rules (Min Conf: {MIN_CONFIDENCE})...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# Delete frequent_itemsets to free memory\n",
    "del frequent_itemsets\n",
    "gc.collect()\n",
    "\n",
    "# 2. Define TARGET_LABELS list\n",
    "TARGET_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "# 3. Create a filter function to check if 'consequents' contains any of the TARGET_LABELS\n",
    "def is_diagnosis_rule(consequents):\n",
    "    return any(label in consequents for label in TARGET_LABELS)\n",
    "\n",
    "# 4. Filter the 'rules' dataframe to keep only rows where consequents are diagnosis labels\n",
    "if not rules.empty:\n",
    "    final_rules = rules[rules['consequents'].apply(is_diagnosis_rule)].copy()\n",
    "else:\n",
    "    final_rules = pd.DataFrame()\n",
    "\n",
    "# 5. Sort the final rules by 'confidence' and 'support' descending\n",
    "if not final_rules.empty:\n",
    "    final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "    # 6. Convert 'antecedents' and 'consequents' columns from frozenset to standard Python lists\n",
    "    final_rules['antecedents'] = final_rules['antecedents'].apply(lambda x: list(x))\n",
    "    final_rules['consequents'] = final_rules['consequents'].apply(lambda x: list(x))\n",
    "\n",
    "    # 7. Print the top 10 rules for preview\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOP 10 PREVIEW (Dari total {len(final_rules)} rules)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for idx, row in final_rules.head(10).iterrows():\n",
    "        print(f\"IF {row['antecedents']} THEN {row['consequents']} (Conf: {row['confidence']:.2f})\")\n",
    "\n",
    "    # 8. Save ALL final rules to OUTPUT_FILE (do not limit to head)\n",
    "    final_rules.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUKSES! Total {len(final_rules)} Rules valid disimpan.\")\n",
    "    print(f\"File: {OUTPUT_FILE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"PERINGATAN: Tidak ada rule diagnosis yang ditemukan!\")\n",
    "    print(\"Cek apakah Label/diagnosis sudah masuk ke transaksi di CELL 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc1373",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Interpretasi\n",
    "\n",
    "### Hasil Analisis\n",
    "Notebook ini menghasilkan association rules yang menunjukkan hubungan antara kondisi jaringan dan tipe anomali. Setiap rule memiliki:\n",
    "- **Antecedents (IF)**: Kondisi atau kombinasi kondisi jaringan yang diamati\n",
    "- **Consequents (THEN)**: Diagnosis/prediksi tipe anomali yang kemungkinan terjadi\n",
    "- **Confidence**: Tingkat akurasi prediksi (semakin tinggi semakin baik)\n",
    "- **Support**: Seberapa sering pola ini muncul dalam data (frekuensi)\n",
    "- **Lift**: Seberapa kuat hubungan antara kondisi dan diagnosis\n",
    "\n",
    "### Penggunaan Praktis\n",
    "Hasil rules ini dapat digunakan untuk:\n",
    "1. **Early Warning System**: Mendeteksi anomali sebelum terjadi dengan mengecek kondisi antecedent\n",
    "2. **Root Cause Analysis**: Memahami faktor-faktor apa yang menyebabkan setiap tipe anomali\n",
    "3. **Network Optimization**: Meningkatkan monitoring dan maintenance protokol berdasarkan insights\n",
    "4. **Decision Support**: Membantu network administrator membuat keputusan yang lebih cepat dan tepat\n",
    "\n",
    "### File Output\n",
    "- **Final_Skripsi_Rules.csv**: Berisi top 10 rules dalam format CSV yang dapat digunakan untuk proses selanjutnya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
