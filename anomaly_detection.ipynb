{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482cd64d",
   "metadata": {},
   "source": [
    "# Analisis Anomali Network Menggunakan FP-Growth dan Association Rules\n",
    "\n",
    "## Deskripsi Proyek\n",
    "Notebook ini menganalisis log jaringan untuk mendeteksi anomali menggunakan algoritma **FP-Growth** (Frequent Pattern Growth) dan **Association Rules**. Tujuan utama adalah menemukan pola hubungan antara kondisi jaringan dan diagnosis anomali.\n",
    "\n",
    "## Struktur Analisis\n",
    "1. **Encoding Data**: Mengonversi data log menjadi format transaksi yang dapat dianalisis\n",
    "2. **FP-Growth Algorithm**: Menemukan pola frekuen dengan threshold minimum support\n",
    "3. **Association Rules**: Menghasilkan aturan asosiasi untuk prediksi diagnosis\n",
    "4. **Filtering & Visualization**: Menyaring aturan yang relevan dan menampilkan hasil akhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b63ad",
   "metadata": {},
   "source": [
    "## Tahap 1: Import Library dan Persiapan Data\n",
    "\n",
    "Pada tahap ini, kami mengimpor library yang diperlukan untuk analisis frequent pattern mining dan memuat dataset yang sudah dibersihkan dari tahap preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c967ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset berhasil dimuat: 6537 baris, 2 kolom\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Import cleaned data dari hasil preprocessing\n",
    "df_cleaned = pd.read_csv('Data/Data_Siap_Mining.csv')\n",
    "print(f\"Dataset berhasil dimuat: {df_cleaned.shape[0]} baris, {df_cleaned.shape[1]} kolom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09efe3",
   "metadata": {},
   "source": [
    "## Tahap 2: Encoding Data dan Perhitungan FP-Growth\n",
    "\n",
    "### Proses Encoding\n",
    "- **TransactionEncoder**: Mengkonversi data log dari format text menjadi format transaksi (one-hot encoded)\n",
    "- Setiap item dalam 'items' column diubah menjadi kolom boolean terpisah\n",
    "- Output: DataFrame `df_encoded` dengan nilai True/False untuk setiap atribut\n",
    "\n",
    "### Perhitungan Support\n",
    "- **Minimum Support**: Threshold minimum untuk pola yang dianggap \"sering\" terjadi\n",
    "- **MIN_SUPPORT_COUNT = 20**: Pola harus muncul minimal 20 kali dalam dataset\n",
    "- **min_support_pct**: Persentase support dihitung dari total jumlah transaksi\n",
    "\n",
    "### Algoritma FP-Growth\n",
    "- **fpgrowth**: Algoritma efisien untuk mining pola frekuen\n",
    "- Lebih cepat dari Apriori karena menggunakan FP-tree structure\n",
    "- Output: `frequent_itemsets` berisi semua pola yang memenuhi threshold\n",
    "\n",
    "### Association Rules\n",
    "- Menghasilkan aturan IF-THEN dari pola frekuen\n",
    "- **min_threshold = 0.6**: Hanya aturan dengan confidence ≥ 60% yang dipertahankan\n",
    "- Metrik yang dihitung: support, confidence, lift, dsb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac89e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data berhasil di-encode:\n",
      "  - Shape: (6537, 127)\n",
      "  - Atribut unik: 127\n",
      "\n",
      "Sample data (5 baris pertama):\n",
      "   0x0004  0x0800    100   1000    130    131    132    133   133-    153  \\\n",
      "0   False   False  False  False  False  False  False  False  False  False   \n",
      "1   False   False  False  False  False  False  False  False  False  False   \n",
      "2   False   False  False  False  False  False  False  False  False  False   \n",
      "3   False   False  False  False  False  False  False  False  False  False   \n",
      "4   False   False  False  False  False  False  False  False  False  False   \n",
      "\n",
      "   ...   type    udp  unknown  up-script  upstream   user  version  waiting  \\\n",
      "0  ...  False  False    False      False     False  False    False    False   \n",
      "1  ...  False  False    False      False     False  False    False    False   \n",
      "2  ...  False  False    False      False     False  False     True    False   \n",
      "3  ...  False  False    False      False     False  False     True    False   \n",
      "4  ...  False  False    False      False     False  False     True    False   \n",
      "\n",
      "   warning    yes  \n",
      "0    False  False  \n",
      "1    False  False  \n",
      "2    False  False  \n",
      "3    False  False  \n",
      "4    False  False  \n",
      "\n",
      "[5 rows x 127 columns]\n",
      "\n",
      "✓ Konfigurasi Minimum Support:\n",
      "  - MIN_SUPPORT_COUNT: 20\n",
      "  - Total Transaksi: 6537\n",
      "  - min_support_pct: 0.0031 (0.31%)\n",
      "\n",
      "✓ FP-Growth selesai:\n",
      "  - Pola frekuen ditemukan: 387065\n",
      "\n",
      "Top 5 frequent itemsets:\n",
      "         support    itemsets\n",
      "0       0.869512   (r1-core)\n",
      "110812  0.835246  (192, 168)\n",
      "38      0.835246       (168)\n",
      "39      0.835246       (192)\n",
      "110816  0.833716  (153, 168)\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 1: ENCODING DATA PER DIAGNOSIS LABEL =====\n",
    "# Untuk mengurangi penggunaan memori, kita akan memproses per diagnosis label\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "# Definisikan label diagnosis yang valid\n",
    "DIAGNOSIS_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "print(f\"✓ Memulai encoding data per diagnosis label untuk menghemat memori...\")\n",
    "print(f\"  - Total dataset: {df_cleaned.shape[0]} baris\")\n",
    "print(f\"  - Diagnosis labels: {DIAGNOSIS_LABELS}\\n\")\n",
    "\n",
    "# Dictionary untuk menyimpan hasil per label\n",
    "encoded_data_per_label = {}\n",
    "te = TransactionEncoder()\n",
    "\n",
    "# Process setiap diagnosis label secara terpisah\n",
    "for label in DIAGNOSIS_LABELS:\n",
    "    print(f\"Processing '{label}'...\", end=\" \")\n",
    "    \n",
    "    # Filter data untuk label ini saja\n",
    "    df_label = df_cleaned[df_cleaned['diagnosis'] == label].copy()\n",
    "    \n",
    "    if len(df_label) == 0:\n",
    "        print(f\"(Skip - tidak ada data)\")\n",
    "        continue\n",
    "    \n",
    "    # Konversi items menjadi list\n",
    "    items_list = df_label['items'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "    \n",
    "    # Transform menggunakan TransactionEncoder\n",
    "    te_ary = te.fit(items_list).transform(items_list)\n",
    "    df_encoded_label = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    # Tambahkan diagnosis label sebagai kolom\n",
    "    df_encoded_label['diagnosis'] = label\n",
    "    \n",
    "    encoded_data_per_label[label] = df_encoded_label\n",
    "    \n",
    "    print(f\"✓ ({len(df_label)} transaksi, {len(df_encoded_label.columns)-1} atribut)\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del df_label, items_list, te_ary\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Encoding selesai untuk semua labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: FP-GROWTH PER DIAGNOSIS LABEL =====\n",
    "# Jalankan FP-Growth untuk masing-masing diagnosis label\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "\n",
    "frequent_itemsets_dict = {}\n",
    "print(f\"✓ Menjalankan FP-Growth per diagnosis label...\")\n",
    "print(f\"  - MIN_SUPPORT_COUNT: {MIN_SUPPORT_COUNT}\\n\")\n",
    "\n",
    "for label, df_encoded in encoded_data_per_label.items():\n",
    "    print(f\"FP-Growth for '{label}'...\", end=\" \")\n",
    "    \n",
    "    # Hitung minimum support percentage\n",
    "    min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "    \n",
    "    # Gunakan hanya kolom atribut (exclude diagnosis column)\n",
    "    df_for_fpgrowth = df_encoded.drop('diagnosis', axis=1)\n",
    "    \n",
    "    # Jalankan FP-Growth\n",
    "    frequent_itemsets = fpgrowth(df_for_fpgrowth, min_support=min_support_pct, use_colnames=True)\n",
    "    \n",
    "    # Tambahkan diagnosis label ke dalam frequent itemsets\n",
    "    frequent_itemsets['diagnosis'] = label\n",
    "    \n",
    "    frequent_itemsets_dict[label] = frequent_itemsets\n",
    "    \n",
    "    print(f\"✓ Ditemukan {len(frequent_itemsets)} pola frekuen\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del df_encoded, df_for_fpgrowth\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✓ FP-Growth selesai untuk semua labels!\")\n",
    "\n",
    "# Gabungkan semua frequent itemsets\n",
    "all_frequent_itemsets = pd.concat(frequent_itemsets_dict.values(), ignore_index=True)\n",
    "print(f\"  - Total pola frekuen: {len(all_frequent_itemsets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: GENERATE ASSOCIATION RULES PER LABEL =====\n",
    "# Buat association rules untuk setiap diagnosis label\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "rules_dict = {}\n",
    "total_rules = 0\n",
    "\n",
    "print(f\"✓ Menghasilkan Association Rules per diagnosis label...\")\n",
    "print(f\"  - MIN_CONFIDENCE: {MIN_CONFIDENCE}\\n\")\n",
    "\n",
    "for label, frequent_itemsets in frequent_itemsets_dict.items():\n",
    "    print(f\"Generating rules for '{label}'...\", end=\" \")\n",
    "    \n",
    "    if len(frequent_itemsets) == 0:\n",
    "        print(\"(Skip - tidak ada frequent itemsets)\")\n",
    "        continue\n",
    "    \n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "    \n",
    "    if len(rules) > 0:\n",
    "        rules['diagnosis'] = label\n",
    "        rules_dict[label] = rules\n",
    "        total_rules += len(rules)\n",
    "        print(f\"✓ Ditemukan {len(rules)} rules\")\n",
    "    else:\n",
    "        print(\"(tidak ada rules dengan confidence >= threshold)\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del frequent_itemsets\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Association Rules generation selesai!\")\n",
    "print(f\"  - Total rules: {total_rules}\")\n",
    "\n",
    "# Gabungkan semua rules\n",
    "if rules_dict:\n",
    "    all_rules = pd.concat(rules_dict.values(), ignore_index=True)\n",
    "    print(f\"  - Rules yang digabungkan: {len(all_rules)}\")\n",
    "else:\n",
    "    all_rules = pd.DataFrame()\n",
    "    print(\"  - Tidak ada rules yang dihasilkan!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e002",
   "metadata": {},
   "source": [
    "## Tahap 3: Penyaringan dan Tampilan Hasil Akhir\n",
    "\n",
    "### Konsep Filtering\n",
    "Dari semua association rules yang dihasilkan, kita hanya ingin menyimpan rules yang:\n",
    "- **Consequent (Kesimpulan)**: Harus berupa diagnosis/label anomali\n",
    "  - NORMAL: Kondisi jaringan normal\n",
    "  - UPSTREAM_FAILURE: Kegagalan uplink\n",
    "  - LINK_FAILURE: Kegagalan link\n",
    "  - DDOS_ATTACK: Serangan DDoS\n",
    "  - BROADCAST_STORM: Broadcast storm\n",
    "\n",
    "Contoh Rule yang BENAR (yang kita inginkan):\n",
    "- IF `ether1=down AND queue=high` THEN `LINK_FAILURE` ✓\n",
    "\n",
    "Contoh Rule yang SALAH (yang kita buang):\n",
    "- IF `LINK_FAILURE` THEN `ether1=down` ✗\n",
    "\n",
    "### Metrik Evaluasi\n",
    "- **Support**: Seberapa sering pola ini muncul dalam dataset (0-1)\n",
    "- **Confidence**: Probabilitas consequent terjadi jika antecedent terjadi (0-1)\n",
    "- **Lift**: Seberapa kuat hubungan antara antecedent dan consequent (>1 = hubungan positif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15793906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: FILTERING RULES =====\n",
    "# Filter rules - hanya ambil yang berakhir dengan diagnosis\n",
    "def is_diagnosis(consequents):\n",
    "    \"\"\"\n",
    "    Cek apakah consequents (frozenset) mengandung minimal 1 label diagnosis\n",
    "    \"\"\"\n",
    "    return any(label in consequents for label in DIAGNOSIS_LABELS)\n",
    "\n",
    "if len(all_rules) > 0:\n",
    "    final_rules = all_rules[all_rules['consequents'].apply(is_diagnosis)].copy()\n",
    "    print(f\"✓ Rules setelah filtering:\")\n",
    "    print(f\"  - Total rules sebelum filter: {len(all_rules)}\")\n",
    "    print(f\"  - Rules dengan diagnosis consequent: {len(final_rules)}\")\n",
    "    print(f\"  - Rules yang dihilangkan: {len(all_rules) - len(final_rules)}\")\n",
    "    \n",
    "    # ===== STEP 5: SORTING =====\n",
    "    # Urutkan berdasarkan confidence (tertinggi) kemudian support (tertinggi)\n",
    "    final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "    print(f\"\\n✓ Rules sudah diurutkan berdasarkan confidence dan support (descending)\")\n",
    "    \n",
    "    # ===== STEP 6: DISPLAY HASIL =====\n",
    "    # Konversi frozenset ke list untuk memudahkan pembacaan CSV dan Streamlit\n",
    "    final_rules_for_display = final_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'diagnosis']].copy()\n",
    "    final_rules_for_display['antecedents'] = final_rules_for_display['antecedents'].apply(list)\n",
    "    final_rules_for_display['consequents'] = final_rules_for_display['consequents'].apply(list)\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"TOP ASSOCIATION RULES - NETWORK ANOMALY DETECTION (PROCESSED PER LABEL)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Tampilkan dengan format yang lebih rapi (hanya 15 untuk display)\n",
    "    for idx, (i, row) in enumerate(final_rules_for_display.head(15).iterrows(), 1):\n",
    "        antecedents = ', '.join(row['antecedents'])\n",
    "        consequents = ', '.join(row['consequents'])\n",
    "        diagnosis = row['diagnosis']\n",
    "        \n",
    "        print(f\"[Rule {idx}] [{diagnosis}]\")\n",
    "        print(f\"  IF:   {antecedents}\")\n",
    "        print(f\"  THEN: {consequents}\")\n",
    "        print(f\"  Support: {row['support']:.4f} | Confidence: {row['confidence']:.4f} | Lift: {row['lift']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # ===== STEP 7: SAVE HASIL =====\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"MENYIMPAN DATA KE CSV...\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Simpan SEMUA final rules ke CSV\n",
    "    final_rules_for_display.to_csv('../Final_Skripsi_Rules.csv', index=False)\n",
    "    print(f\"✓ SEMUA {len(final_rules_for_display)} rules berhasil disimpan ke 'Final_Skripsi_Rules.csv'\")\n",
    "    print(f\"  - File ini akan digunakan oleh Dashboard Live Monitoring\")\n",
    "    print(f\"  - Frozenset sudah dikonversi ke list untuk kompatibilitas Streamlit\")\n",
    "    print(f\"  - Format: antecedents | consequents | support | confidence | lift | diagnosis\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"✓ TOP 15 RULES (sample dari {len(final_rules_for_display)} total rules)\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Bersihkan memori\n",
    "    del all_rules, all_frequent_itemsets\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ Tidak ada rules yang dihasilkan dari semua labels!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc1373",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Interpretasi\n",
    "\n",
    "### Hasil Analisis\n",
    "Notebook ini menghasilkan association rules yang menunjukkan hubungan antara kondisi jaringan dan tipe anomali. Setiap rule memiliki:\n",
    "- **Antecedents (IF)**: Kondisi atau kombinasi kondisi jaringan yang diamati\n",
    "- **Consequents (THEN)**: Diagnosis/prediksi tipe anomali yang kemungkinan terjadi\n",
    "- **Confidence**: Tingkat akurasi prediksi (semakin tinggi semakin baik)\n",
    "- **Support**: Seberapa sering pola ini muncul dalam data (frekuensi)\n",
    "- **Lift**: Seberapa kuat hubungan antara kondisi dan diagnosis\n",
    "\n",
    "### Penggunaan Praktis\n",
    "Hasil rules ini dapat digunakan untuk:\n",
    "1. **Early Warning System**: Mendeteksi anomali sebelum terjadi dengan mengecek kondisi antecedent\n",
    "2. **Root Cause Analysis**: Memahami faktor-faktor apa yang menyebabkan setiap tipe anomali\n",
    "3. **Network Optimization**: Meningkatkan monitoring dan maintenance protokol berdasarkan insights\n",
    "4. **Decision Support**: Membantu network administrator membuat keputusan yang lebih cepat dan tepat\n",
    "\n",
    "### File Output\n",
    "- **Final_Skripsi_Rules.csv**: Berisi top 10 rules dalam format CSV yang dapat digunakan untuk proses selanjutnya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
