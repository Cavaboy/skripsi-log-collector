{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb06546",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.9' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup & Import\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Konfigurasi File\n",
    "INPUT_FILE = 'Data/Data_Siap_Mining.csv'\n",
    "OUTPUT_FILE = 'Final_Skripsi_Rules.csv'\n",
    "\n",
    "# Konfigurasi Algoritma\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "print(\"‚úÖ Library loaded & Config set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cd64d",
   "metadata": {},
   "source": [
    "# Analisis Anomali Network Menggunakan FP-Growth dan Association Rules\n",
    "\n",
    "## Deskripsi Proyek\n",
    "Notebook ini menganalisis log jaringan untuk mendeteksi anomali menggunakan algoritma **FP-Growth** (Frequent Pattern Growth) dan **Association Rules**. Tujuan utama adalah menemukan pola hubungan antara kondisi jaringan dan diagnosis anomali.\n",
    "\n",
    "## Struktur Analisis\n",
    "1. **Encoding Data**: Mengonversi data log menjadi format transaksi yang dapat dianalisis\n",
    "2. **FP-Growth Algorithm**: Menemukan pola frekuen dengan threshold minimum support\n",
    "3. **Association Rules**: Menghasilkan aturan asosiasi untuk prediksi diagnosis\n",
    "4. **Filtering & Visualization**: Menyaring aturan yang relevan dan menampilkan hasil akhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b63ad",
   "metadata": {},
   "source": [
    "## Tahap 1: Import Library dan Persiapan Data\n",
    "\n",
    "Pada tahap ini, kami mengimpor library yang diperlukan untuk analisis frequent pattern mining dan memuat dataset yang sudah dibersihkan dari tahap preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09efe3",
   "metadata": {},
   "source": [
    "## Tahap 2: Encoding Data dan Perhitungan FP-Growth\n",
    "\n",
    "### Proses Encoding\n",
    "- **TransactionEncoder**: Mengkonversi data log dari format text menjadi format transaksi (one-hot encoded)\n",
    "- Setiap item dalam 'items' column diubah menjadi kolom boolean terpisah\n",
    "- Output: DataFrame `df_encoded` dengan nilai True/False untuk setiap atribut\n",
    "\n",
    "### Perhitungan Support\n",
    "- **Minimum Support**: Threshold minimum untuk pola yang dianggap \"sering\" terjadi\n",
    "- **MIN_SUPPORT_COUNT = 20**: Pola harus muncul minimal 20 kali dalam dataset\n",
    "- **min_support_pct**: Persentase support dihitung dari total jumlah transaksi\n",
    "\n",
    "### Algoritma FP-Growth\n",
    "- **fpgrowth**: Algoritma efisien untuk mining pola frekuen\n",
    "- Lebih cepat dari Apriori karena menggunakan FP-tree structure\n",
    "- Output: `frequent_itemsets` berisi semua pola yang memenuhi threshold\n",
    "\n",
    "### Association Rules\n",
    "- Menghasilkan aturan IF-THEN dari pola frekuen\n",
    "- **min_threshold = 0.6**: Hanya aturan dengan confidence ‚â• 60% yang dipertahankan\n",
    "- Metrik yang dihitung: support, confidence, lift, dsb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e002",
   "metadata": {},
   "source": [
    "## Tahap 3: Penyaringan dan Tampilan Hasil Akhir\n",
    "\n",
    "### Konsep Filtering\n",
    "Dari semua association rules yang dihasilkan, kita hanya ingin menyimpan rules yang:\n",
    "- **Consequent (Kesimpulan)**: Harus berupa diagnosis/label anomali\n",
    "  - NORMAL: Kondisi jaringan normal\n",
    "  - UPSTREAM_FAILURE: Kegagalan uplink\n",
    "  - LINK_FAILURE: Kegagalan link\n",
    "  - DDOS_ATTACK: Serangan DDoS\n",
    "  - BROADCAST_STORM: Broadcast storm\n",
    "\n",
    "Contoh Rule yang BENAR (yang kita inginkan):\n",
    "- IF `ether1=down AND queue=high` THEN `LINK_FAILURE` ‚úì\n",
    "\n",
    "Contoh Rule yang SALAH (yang kita buang):\n",
    "- IF `LINK_FAILURE` THEN `ether1=down` ‚úó\n",
    "\n",
    "### Metrik Evaluasi\n",
    "- **Support**: Seberapa sering pola ini muncul dalam dataset (0-1)\n",
    "- **Confidence**: Probabilitas consequent terjadi jika antecedent terjadi (0-1)\n",
    "- **Lift**: Seberapa kuat hubungan antara antecedent dan consequent (>1 = hubungan positif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc1373",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Interpretasi\n",
    "\n",
    "### Hasil Analisis\n",
    "Notebook ini menghasilkan association rules yang menunjukkan hubungan antara kondisi jaringan dan tipe anomali. Setiap rule memiliki:\n",
    "- **Antecedents (IF)**: Kondisi atau kombinasi kondisi jaringan yang diamati\n",
    "- **Consequents (THEN)**: Diagnosis/prediksi tipe anomali yang kemungkinan terjadi\n",
    "- **Confidence**: Tingkat akurasi prediksi (semakin tinggi semakin baik)\n",
    "- **Support**: Seberapa sering pola ini muncul dalam data (frekuensi)\n",
    "- **Lift**: Seberapa kuat hubungan antara kondisi dan diagnosis\n",
    "\n",
    "### Penggunaan Praktis\n",
    "Hasil rules ini dapat digunakan untuk:\n",
    "1. **Early Warning System**: Mendeteksi anomali sebelum terjadi dengan mengecek kondisi antecedent\n",
    "2. **Root Cause Analysis**: Memahami faktor-faktor apa yang menyebabkan setiap tipe anomali\n",
    "3. **Network Optimization**: Meningkatkan monitoring dan maintenance protokol berdasarkan insights\n",
    "4. **Decision Support**: Membantu network administrator membuat keputusan yang lebih cepat dan tepat\n",
    "\n",
    "### File Output\n",
    "- **Final_Skripsi_Rules.csv**: Berisi top 10 rules dalam format CSV yang dapat digunakan untuk proses selanjutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Load Data & Gabung Label (CRITICAL STEP)\n",
    "# 1. Load the dataset from INPUT_FILE\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"üìä Dataset dimuat: {len(df)} baris, {df.shape[1]} kolom\")\n",
    "\n",
    "# 2. Determine if the label column is named 'Label' or 'diagnosis'\n",
    "label_col = 'Label' if 'Label' in df.columns else 'diagnosis'\n",
    "print(f\"‚ÑπÔ∏è Kolom Label: {label_col}\")\n",
    "\n",
    "# 3. Create a list called 'transactions'\n",
    "# 4. For each row, parse the 'items' string using ast.literal_eval (NOT eval)\n",
    "# 5. CRITICAL: Append the row's Label/diagnosis value into the items list\n",
    "# 6. Add the list to 'transactions'\n",
    "transactions = []\n",
    "for _, row in df.iterrows():\n",
    "    # Parse string list kembali ke list python menggunakan ast.literal_eval (AMAN)\n",
    "    items = ast.literal_eval(row['items']) if isinstance(row['items'], str) else row['items']\n",
    "    # CRITICAL: Masukkan Label ke dalam items list (INI KUNCINYA!)\n",
    "    items.append(row[label_col])\n",
    "    transactions.append(items)\n",
    "\n",
    "# 7. Delete the dataframe and run gc.collect() to free up memory\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# 8. Print the first transaction to verify the label is included\n",
    "print(f\"‚úì Transaksi siap. Contoh baris pertama: {transactions[0]}\")\n",
    "print(f\"‚úì Total transaksi: {len(transactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Encoding & FP-Growth\n",
    "# 1. Initialize TransactionEncoder. Fit and transform 'transactions' into a boolean array\n",
    "print(\"‚è≥ Sedang melakukan Encoding...\")\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# 2. Delete 'transactions' list and run gc.collect() immediately to save RAM\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "# 3. Create a DataFrame 'df_encoded' from the array. Delete the array and run gc.collect()\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "del te_ary\n",
    "gc.collect()\n",
    "\n",
    "# 4. Calculate 'min_support_pct' by dividing MIN_SUPPORT_COUNT by the length of df_encoded\n",
    "min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "print(f\"‚úì Encoding Selesai. Shape: {df_encoded.shape}\")\n",
    "print(f\"üîç Mining dengan Min Support: {min_support_pct:.5f} (Min {MIN_SUPPORT_COUNT} kejadian)\")\n",
    "\n",
    "# 5. Run fpgrowth on 'df_encoded' using 'min_support_pct' and use_colnames=True\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support_pct, use_colnames=True)\n",
    "\n",
    "# 6. Delete 'df_encoded' and run gc.collect()\n",
    "del df_encoded\n",
    "gc.collect()\n",
    "\n",
    "# 7. Print the number of frequent itemsets found\n",
    "print(f\"‚úì Mining Selesai. Ditemukan {len(frequent_itemsets)} pola itemset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b636bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Generate Rules, Filter & Save\n",
    "# 1. Generate association rules using metric=\"confidence\" and min_threshold=MIN_CONFIDENCE\n",
    "print(f\"\\n‚öôÔ∏è Generating Rules (Min Conf: {MIN_CONFIDENCE})...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# Delete frequent_itemsets to free memory\n",
    "del frequent_itemsets\n",
    "gc.collect()\n",
    "\n",
    "# 2. Define TARGET_LABELS list\n",
    "TARGET_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "# 3. Create a filter function to check if 'consequents' contains any of the TARGET_LABELS\n",
    "def is_diagnosis_rule(consequents):\n",
    "    return any(label in consequents for label in TARGET_LABELS)\n",
    "\n",
    "# 4. Filter the 'rules' dataframe to keep only rows where consequents are diagnosis labels\n",
    "if not rules.empty:\n",
    "    final_rules = rules[rules['consequents'].apply(is_diagnosis_rule)].copy()\n",
    "else:\n",
    "    final_rules = pd.DataFrame()\n",
    "\n",
    "# 5. Sort the final rules by 'confidence' and 'support' descending\n",
    "if not final_rules.empty:\n",
    "    final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "    # 6. Convert 'antecedents' and 'consequents' columns from frozenset to standard Python lists\n",
    "    final_rules['antecedents'] = final_rules['antecedents'].apply(lambda x: list(x))\n",
    "    final_rules['consequents'] = final_rules['consequents'].apply(lambda x: list(x))\n",
    "\n",
    "    # 7. Print the top 10 rules for preview\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOP 10 PREVIEW (Dari total {len(final_rules)} rules)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for idx, row in final_rules.head(10).iterrows():\n",
    "        print(f\"IF {row['antecedents']} THEN {row['consequents']} (Conf: {row['confidence']:.2f})\")\n",
    "\n",
    "    # 8. Save ALL final rules to OUTPUT_FILE (do not limit to head)\n",
    "    final_rules.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ SUKSES! Total {len(final_rules)} Rules valid disimpan.\")\n",
    "    print(f\"üíæ File: {OUTPUT_FILE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå PERINGATAN: Tidak ada rule diagnosis yang ditemukan!\")\n",
    "    print(\"   Cek apakah Label/diagnosis sudah masuk ke transaksi di CELL 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
