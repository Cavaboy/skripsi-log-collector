{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb06546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import & Konfigurasi\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Konfigurasi File\n",
    "INPUT_FILE = 'Data/Data_Siap_Mining.csv' # Sesuaikan path jika perlu\n",
    "OUTPUT_FILE = 'Final_Skripsi_Rules.csv'\n",
    "\n",
    "# Konfigurasi Algoritma\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "print(\"‚úÖ Library loaded & Config set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cd64d",
   "metadata": {},
   "source": [
    "# Analisis Anomali Network Menggunakan FP-Growth dan Association Rules\n",
    "\n",
    "## Deskripsi Proyek\n",
    "Notebook ini menganalisis log jaringan untuk mendeteksi anomali menggunakan algoritma **FP-Growth** (Frequent Pattern Growth) dan **Association Rules**. Tujuan utama adalah menemukan pola hubungan antara kondisi jaringan dan diagnosis anomali.\n",
    "\n",
    "## Struktur Analisis\n",
    "1. **Encoding Data**: Mengonversi data log menjadi format transaksi yang dapat dianalisis\n",
    "2. **FP-Growth Algorithm**: Menemukan pola frekuen dengan threshold minimum support\n",
    "3. **Association Rules**: Menghasilkan aturan asosiasi untuk prediksi diagnosis\n",
    "4. **Filtering & Visualization**: Menyaring aturan yang relevan dan menampilkan hasil akhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b63ad",
   "metadata": {},
   "source": [
    "## Tahap 1: Import Library dan Persiapan Data\n",
    "\n",
    "Pada tahap ini, kami mengimpor library yang diperlukan untuk analisis frequent pattern mining dan memuat dataset yang sudah dibersihkan dari tahap preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09efe3",
   "metadata": {},
   "source": [
    "## Tahap 2: Encoding Data dan Perhitungan FP-Growth\n",
    "\n",
    "### Proses Encoding\n",
    "- **TransactionEncoder**: Mengkonversi data log dari format text menjadi format transaksi (one-hot encoded)\n",
    "- Setiap item dalam 'items' column diubah menjadi kolom boolean terpisah\n",
    "- Output: DataFrame `df_encoded` dengan nilai True/False untuk setiap atribut\n",
    "\n",
    "### Perhitungan Support\n",
    "- **Minimum Support**: Threshold minimum untuk pola yang dianggap \"sering\" terjadi\n",
    "- **MIN_SUPPORT_COUNT = 20**: Pola harus muncul minimal 20 kali dalam dataset\n",
    "- **min_support_pct**: Persentase support dihitung dari total jumlah transaksi\n",
    "\n",
    "### Algoritma FP-Growth\n",
    "- **fpgrowth**: Algoritma efisien untuk mining pola frekuen\n",
    "- Lebih cepat dari Apriori karena menggunakan FP-tree structure\n",
    "- Output: `frequent_itemsets` berisi semua pola yang memenuhi threshold\n",
    "\n",
    "### Association Rules\n",
    "- Menghasilkan aturan IF-THEN dari pola frekuen\n",
    "- **min_threshold = 0.6**: Hanya aturan dengan confidence ‚â• 60% yang dipertahankan\n",
    "- Metrik yang dihitung: support, confidence, lift, dsb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e002",
   "metadata": {},
   "source": [
    "## Tahap 3: Penyaringan dan Tampilan Hasil Akhir\n",
    "\n",
    "### Konsep Filtering\n",
    "Dari semua association rules yang dihasilkan, kita hanya ingin menyimpan rules yang:\n",
    "- **Consequent (Kesimpulan)**: Harus berupa diagnosis/label anomali\n",
    "  - NORMAL: Kondisi jaringan normal\n",
    "  - UPSTREAM_FAILURE: Kegagalan uplink\n",
    "  - LINK_FAILURE: Kegagalan link\n",
    "  - DDOS_ATTACK: Serangan DDoS\n",
    "  - BROADCAST_STORM: Broadcast storm\n",
    "\n",
    "Contoh Rule yang BENAR (yang kita inginkan):\n",
    "- IF `ether1=down AND queue=high` THEN `LINK_FAILURE` ‚úì\n",
    "\n",
    "Contoh Rule yang SALAH (yang kita buang):\n",
    "- IF `LINK_FAILURE` THEN `ether1=down` ‚úó\n",
    "\n",
    "### Metrik Evaluasi\n",
    "- **Support**: Seberapa sering pola ini muncul dalam dataset (0-1)\n",
    "- **Confidence**: Probabilitas consequent terjadi jika antecedent terjadi (0-1)\n",
    "- **Lift**: Seberapa kuat hubungan antara antecedent dan consequent (>1 = hubungan positif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc1373",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Interpretasi\n",
    "\n",
    "### Hasil Analisis\n",
    "Notebook ini menghasilkan association rules yang menunjukkan hubungan antara kondisi jaringan dan tipe anomali. Setiap rule memiliki:\n",
    "- **Antecedents (IF)**: Kondisi atau kombinasi kondisi jaringan yang diamati\n",
    "- **Consequents (THEN)**: Diagnosis/prediksi tipe anomali yang kemungkinan terjadi\n",
    "- **Confidence**: Tingkat akurasi prediksi (semakin tinggi semakin baik)\n",
    "- **Support**: Seberapa sering pola ini muncul dalam data (frekuensi)\n",
    "- **Lift**: Seberapa kuat hubungan antara kondisi dan diagnosis\n",
    "\n",
    "### Penggunaan Praktis\n",
    "Hasil rules ini dapat digunakan untuk:\n",
    "1. **Early Warning System**: Mendeteksi anomali sebelum terjadi dengan mengecek kondisi antecedent\n",
    "2. **Root Cause Analysis**: Memahami faktor-faktor apa yang menyebabkan setiap tipe anomali\n",
    "3. **Network Optimization**: Meningkatkan monitoring dan maintenance protokol berdasarkan insights\n",
    "4. **Decision Support**: Membantu network administrator membuat keputusan yang lebih cepat dan tepat\n",
    "\n",
    "### File Output\n",
    "- **Final_Skripsi_Rules.csv**: Berisi top 10 rules dalam format CSV yang dapat digunakan untuk proses selanjutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 1: LOAD & PREPARE TRANSACTIONS =====\n",
    "import pandas as pd\n",
    "import ast\n",
    "import gc\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Load data yang sudah bersih\n",
    "df_cleaned = pd.read_csv('Data/Data_Siap_Mining.csv')\n",
    "print(f\"üìä Dataset dimuat: {len(df_cleaned)} baris, {df_cleaned.shape[1]} kolom\")\n",
    "\n",
    "# Deteksi nama kolom label (antisipasi beda nama)\n",
    "label_col = 'Label' if 'Label' in df_cleaned.columns else 'diagnosis'\n",
    "print(f\"‚ÑπÔ∏è Menggunakan kolom label: {label_col}\")\n",
    "\n",
    "# GABUNGKAN Gejala (Items) + Label (Diagnosis) -> INI KUNCINYA!\n",
    "transactions = []\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    # Ubah string log kembali jadi list (GUNAKAN ast.literal_eval - LEBIH AMAN)\n",
    "    items = ast.literal_eval(row['items']) if isinstance(row['items'], str) else row['items']\n",
    "    # MASUKKAN Label Diagnosis ke dalam list agar ikut dipelajari\n",
    "    items.append(row[label_col])\n",
    "    transactions.append(items)\n",
    "\n",
    "print(f\"‚úì Transaksi siap. Contoh: {transactions[0]}\")\n",
    "\n",
    "# Hapus DF awal untuk hemat memori\n",
    "del df_cleaned\n",
    "gc.collect()\n",
    "\n",
    "# ===== STEP 2: ENCODING =====\n",
    "print(\"\\n‚è≥ Sedang melakukan Encoding...\")\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# Hapus list transaksi segera\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Hapus array numpy segera\n",
    "del te_ary\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úì Encoding Selesai. Shape Matriks: {df_encoded.shape}\")\n",
    "\n",
    "# ===== STEP 3: FP-GROWTH MINING =====\n",
    "MIN_SUPPORT_COUNT = 20\n",
    "MIN_CONFIDENCE = 0.6\n",
    "min_support_pct = MIN_SUPPORT_COUNT / len(df_encoded)\n",
    "print(f\"\\nüîç Mining dengan Min Support: {min_support_pct:.5f} (Min {MIN_SUPPORT_COUNT} kejadian)\")\n",
    "\n",
    "# Jalankan FP-Growth\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support_pct, use_colnames=True)\n",
    "\n",
    "# Hapus matriks besar segera (Hemat RAM)\n",
    "del df_encoded\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úì Mining Selesai. Ditemukan {len(frequent_itemsets)} pola itemset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 4: GENERATE RULES =====\n",
    "print(f\"\\n‚öôÔ∏è Generating Rules (Min Conf: {MIN_CONFIDENCE})...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# Bersihkan memori\n",
    "del frequent_itemsets\n",
    "gc.collect()\n",
    "\n",
    "# ===== STEP 5: FILTERING DIAGNOSIS =====\n",
    "# Hanya simpan rule yang ujungnya (Consequents) adalah Diagnosis\n",
    "TARGET_LABELS = ['NORMAL', 'UPSTREAM_FAILURE', 'LINK_FAILURE', 'DDOS_ATTACK', 'BROADCAST_STORM']\n",
    "\n",
    "def is_diagnosis_rule(consequents):\n",
    "    return any(label in consequents for label in TARGET_LABELS)\n",
    "\n",
    "if not rules.empty:\n",
    "    final_rules = rules[rules['consequents'].apply(is_diagnosis_rule)].copy()\n",
    "else:\n",
    "    final_rules = pd.DataFrame()\n",
    "\n",
    "# ===== STEP 6: FORMATTING & SAVING =====\n",
    "if not final_rules.empty:\n",
    "    # Sort dari yang paling akurat\n",
    "    final_rules = final_rules.sort_values(['confidence', 'support'], ascending=[False, False])\n",
    "\n",
    "    # Ubah format frozenset jadi list biasa (Wajib untuk Dashboard)\n",
    "    final_rules['antecedents'] = final_rules['antecedents'].apply(lambda x: list(x))\n",
    "    final_rules['consequents'] = final_rules['consequents'].apply(lambda x: list(x))\n",
    "\n",
    "    # Tampilkan Top 10 (Hanya untuk dilihat mata)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TOP 10 PREVIEW (Dari total {len(final_rules)} rules)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for idx, row in final_rules.head(10).iterrows():\n",
    "        print(f\"IF {row['antecedents']} THEN {row['consequents']} (Conf: {row['confidence']:.2f})\")\n",
    "\n",
    "    # SIMPAN SEMUA RULES KE CSV (PENTING: Jangan pakai .head() disini!)\n",
    "    output_path = 'Final_Skripsi_Rules.csv'\n",
    "    final_rules.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"‚úÖ SUKSES! Total {len(final_rules)} Rules valid disimpan.\")\n",
    "    print(f\"üíæ File: {output_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå PERINGATAN: Tidak ada rule diagnosis yang ditemukan! Cek apakah label sudah masuk ke transaksi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
